# Usamos una imagen base con ROCm y Ubuntu 24.04
FROM rocm/dev-ubuntu-24.04:6.4-complete

# Actualizamos el sistema
RUN apt-get update && apt-get upgrade -y

# Crear directorio para las claves GPG
RUN sudo mkdir -p /etc/apt/keyrings

# Descargar y agregar la nueva clave GPG de ROCm
RUN curl -fsSL https://repo.radeon.com/rocm/rocm.gpg.key | \
    gpg --dearmor | \
    sudo tee /etc/apt/keyrings/rocm.gpg > /dev/null

# Agregar el repositorio de ROCm a las fuentes APT
RUN echo "deb [arch=amd64 signed-by=/etc/apt/keyrings/rocm.gpg] https://repo.radeon.com/rocm/apt/7.0.2 noble main" | \
    sudo tee /etc/apt/sources.list.d/rocm.list > /dev/null

# Instalamos las dependencias necesarias
RUN apt-get update && apt-get install -y \
    cmake \
    make \
    git \
    wget \
    build-essential \
    libvulkan-dev \
    python3-pip \
    python3-dev \
    libcurl4-openssl-dev \
    libtcmalloc-minimal4 \
    libprotobuf-dev \
    clang \
    gcc && rm -rf /var/lib/apt/lists/*

# Instalación de ROCm DKMS
RUN apt-get update && apt-get install -y rocm-dkms

# Variables de entorno para ROCm y Vulkan
ENV MAX_JOBS=14
ENV ROCM_ARCH=gfx803
ENV PYTORCH_ROCM_ARCH=gfx803
ENV TORCH_BLAS_PREFER_HIPBLASLT=0
ENV ROC_ENABLE_PRE_VEGA=1
ENV USE_CUDA=0
ENV USE_ROCM=1
ENV USE_NINJA=1
ENV FORCE_CUDA=1
ENV JOBLIB_START_METHOD=thread
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONENCODING=UTF-8
ENV REQS_FILE=requirements.txt
ENV PIP_ROOT_USER_ACTION=ignore

# Clonamos el repositorio de Ollama
RUN git clone https://github.com/ollama/ollama.git /ollama

# Reemplazamos la configuración para gfx803
WORKDIR /ollama
RUN sed -i 's/var RocmComputeMajorMin = "9"/var RocmComputeMajorMin = "8"/' discover/gpu.go && \
    sed -i 's/find_package(hip REQUIRED)/#find_package(hip REQUIRED)/' CMakeLists.txt && \
    true

# Instalamos las dependencias de Python
COPY requirements.txt /ollama/requirements.txt
RUN pip install -r /ollama/requirements.txt

# Compilamos Ollama para gfx803
RUN cmake -B build -DAMDGPU_TARGETS="${ROCM_ARCH}" && \
    cmake --build build -- -j${MAX_JOBS} && \
    go generate ./... && \
    go build -p ${MAX_JOBS} . && \
    true

# Configuramos el script de inicio
RUN touch /ollama/ol_serve.sh && \
    echo "#!/bin/bash" > /ollama/ol_serve.sh && \
    echo "./ollama serve" >> /ollama/ol_serve.sh && \
    chmod +x /ollama/ol_serve.sh

# Exponemos los puertos necesarios
EXPOSE 8080 11434

# Ejecutamos el script de inicio de Ollama
ENTRYPOINT ["/ollama/ol_serve.sh"]
