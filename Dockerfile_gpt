# Usamos una imagen base con Ubuntu 24.04
FROM ubuntu:24.04

# Establecemos las variables de entorno para el contenedor
ENV DEBIAN_FRONTEND=noninteractive

# Actualizamos el sistema e instalamos dependencias b치sicas
RUN apt-get update && apt-get upgrade -y && apt-get install -y \
    build-essential \
    libvulkan-dev \
    python3-pip \
    python3-dev \
    libcurl4-openssl-dev \
    libtcmalloc-minimal4 \
    libprotobuf-dev \
    clang \
    gcc \
    wget \
    git \
    cmake \
    make \
    software-properties-common

# Creamos el directorio para las claves GPG
RUN mkdir -p /etc/apt/keyrings

# Descargar y agregar la clave GPG de ROCm
RUN curl -fsSL https://repo.radeon.com/rocm/rocm.asc | gpg --dearmor | tee /etc/apt/keyrings/rocm.asc > /dev/null

# Agregar el repositorio de ROCm para Ubuntu 24.04
RUN echo "deb [arch=amd64 signed-by=/etc/apt/keyrings/rocm.asc] https://repo.radeon.com/rocm/apt/7.0.2 noble main" | tee /etc/apt/sources.list.d/rocm.list > /dev/null

# Actualizamos APT y a침adimos los paquetes de ROCm
RUN apt-get update && apt-get install -y \
    rocm-dkms \
    rocm-dev \
    rocm-utils

# Instalamos Python y las dependencias de Ollama
COPY requirements.txt /tmp/requirements.txt
RUN pip3 install -r /tmp/requirements.txt

# Clonamos el repositorio de Ollama
RUN git clone https://github.com/ollama/ollama.git /ollama

# Reemplazamos la configuraci칩n para gfx803 (RX 580)
WORKDIR /ollama
RUN sed -i 's/var RocmComputeMajorMin = "9"/var RocmComputeMajorMin = "8"/' discover/gpu.go && \
    sed -i 's/find_package(hip REQUIRED)/#find_package(hip REQUIRED)/' CMakeLists.txt && \
    true

# Instalamos las dependencias de Python necesarias para Ollama
RUN pip3 install -r /ollama/requirements.txt

# Compilamos Ollama para la GPU RX 580
RUN cmake -B build -DAMDGPU_TARGETS="gfx803" && \
    cmake --build build -- -j$(nproc) && \
    go generate ./... && \
    go build -p $(nproc) . && \
    true

# Creamos el script de inicio
RUN echo "#!/bin/bash" > /ollama/ol_serve.sh && \
    echo "./ollama serve" >> /ollama/ol_serve.sh && \
    chmod +x /ollama/ol_serve.sh

# Exponemos los puertos necesarios para la comunicaci칩n
EXPOSE 8080 11434

# Ejecutamos el script de inicio de Ollama
ENTRYPOINT ["/ollama/ol_serve.sh"]
