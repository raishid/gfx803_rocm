# Utilizamos la imagen oficial de ROCm para Ubuntu 24.04
FROM rocm/dev-ubuntu-24.04:6.4-complete

# Establecemos las variables de entorno para el contenedor
ENV DEBIAN_FRONTEND=noninteractive

# Actualizamos el sistema e instalamos las dependencias necesarias
RUN apt-get update && apt-get upgrade -y && apt-get install -y \
    build-essential \
    libvulkan-dev \
    python3-pip \
    python3-dev \
    libcurl4-openssl-dev \
    libtcmalloc-minimal4 \
    libprotobuf-dev \
    clang \
    gcc \
    wget \
    git \
    cmake \
    make \
    software-properties-common

# Instalamos las dependencias de Python necesarias
COPY requirements.txt /tmp/requirements.txt
RUN pip3 install -r /tmp/requirements.txt

# Clonamos el repositorio de Ollama
RUN git clone https://github.com/ollama/ollama.git /ollama

# Reemplazamos la configuración para gfx803 (RX 580)
WORKDIR /ollama
RUN sed -i 's/var RocmComputeMajorMin = "9"/var RocmComputeMajorMin = "8"/' discover/gpu.go && \
    sed -i 's/find_package(hip REQUIRED)/#find_package(hip REQUIRED)/' CMakeLists.txt && \
    true

# Instalamos las dependencias de Python necesarias para Ollama
RUN pip3 install -r /ollama/requirements.txt

# Compilamos Ollama para la GPU RX 580
RUN cmake -B build -DAMDGPU_TARGETS="gfx803" && \
    cmake --build build -- -j$(nproc) && \
    go generate ./... && \
    go build -p $(nproc) . && \
    true

# Creamos el script de inicio
RUN echo "#!/bin/bash" > /ollama/ol_serve.sh && \
    echo "./ollama serve" >> /ollama/ol_serve.sh && \
    chmod +x /ollama/ol_serve.sh

# Exponemos los puertos necesarios para la comunicación
EXPOSE 8080 11434

# Ejecutamos el script de inicio de Ollama
ENTRYPOINT ["/ollama/ol_serve.sh"]
